# Problem Research Round 2: Rubicon

8 self-contained prompts. Copy each one into a separate Deep Research session. Save results to `rubicon/research/round2/` using the filename listed at the top of each prompt.

---

## Prompt 1

**Save as:** `decision-science.md`

---

I'm building a problem thesis for Rubicon, an AI-powered due diligence platform for private equity, investment banking, and M&A professionals. This is Round 2 research — going deeper on specific gaps from an initial research round.

**The core thesis:** Due diligence fails because it requires cross-document synthesis of thousands of unstructured pages within deal-driven timelines that structurally exceed human cognitive capacity. Current tools (virtual data rooms) store documents securely but do nothing to make them analytically accessible.

**This prompt focuses on:** Decision science and expert cognitive performance under information overload.

**What prior research already established (do not re-find these — go deeper):**
- Hahn, Lawson & Young (1992): inverted U-shaped relationship between information load and decision quality under time pressure
- Moore (2017): under time pressure, people gather less information, rely on heuristics, are less accurate
- Frontiers in Psychology systematic review (2023): information overload linked to strain, burnout, performance losses
- Phillips-Wren & Adya (2020): information overload, time pressure, complexity, and uncertainty as decision stressors

**The gap:** These findings are mostly general psychology or student experiments. I need research on EXPERT decision-making under information overload — specifically professionals (lawyers, accountants, financial analysts, doctors, intelligence analysts) with domain expertise processing large volumes of information under time pressure.

**Find:**
- Academic research on expert decision-making under information overload using professional samples (not students)
- Studies from medical diagnosis, legal judgment, financial analysis, or intelligence analysis — domains where experts process large information volumes under time pressure
- Research on cognitive biases specific to expert review: anchoring, confirmation bias, satisficing under time pressure. Does expertise protect against overload or create new failure modes (overconfidence)?
- Research on the specific cognitive failure mode in document review — why do trained professionals miss things they're looking for?
- Studies on the relationship between document volume and error rate in professional settings
- Research on "cognitive tunneling," "attention narrowing," or "inattentional blindness" under high workload
- The concept of "satisficing" in professional review — when experts stop searching because they've found "enough"
- Any research comparing expert vs. novice performance degradation under information overload

**Sources to prioritize:**
- Journal of Behavioral Decision Making
- Organizational Behavior and Human Decision Processes
- Journal of Financial Economics (behavioral finance)
- Medical decision-making literature (closest analogy to DD — high stakes, time pressure, information overload)
- Intelligence analysis research (CIA, RAND studies on analytic failure)
- Legal scholarship on cognitive biases in contract review
- Human Factors and Ergonomics research
- Audit and accounting research on review quality under workload

**For each finding, provide:**
- Claim: [the finding]
- Source: [author/org name]
- Date: [publication date]
- URL: [url where available]
- Quote: [verbatim excerpt if available]
- Relevance: [how this applies to due diligence specifically]

**Do NOT research:** Market size, TAM/SAM/SOM, dollar figures, number of potential users, industry revenue, or market growth trends. Focus only on the cognitive science.

---

## Prompt 2

**Save as:** `post-close-failures.md`

---

I'm building a problem thesis for Rubicon, an AI-powered due diligence platform for private equity, investment banking, and M&A professionals. This is Round 2 research — going deeper on specific gaps from an initial research round.

**The core thesis:** Due diligence fails because it requires cross-document synthesis of thousands of unstructured pages within deal-driven timelines that structurally exceed human cognitive capacity. Current tools (virtual data rooms) store documents securely but do nothing to make them analytically accessible.

**This prompt focuses on:** Post-close M&A failures — the anatomy of what specifically gets missed during due diligence.

**What prior research already established (do not re-find these — go deeper):**
- 30-38% of private M&A deals generate post-close indemnification claims (SRS Acquiom, 2020/2022)
- Financial statement breaches = 37% of RWI losses paid; material contracts = 31% (Fasken/Euclid/AIG)
- 84% of financial statement breach claims involve companies with audited financials (Euclid Transactional, 2023)
- 49% of RWI claims surface 12+ months post-close (Fasken)
- "No Undisclosed Liabilities" claims surging (SRS Acquiom, 2025)
- 70-75% of acquisitions fail to create shareholder value (Fortune/Lev & Gu, 2024)
- HP/Autonomy and Caterpillar/Siwei are the two detailed case studies we have

**The gap:** We have aggregate statistics but not the anatomy of failure. What specific types of issues get missed? Was the information available in the data room but not connected, or genuinely hidden? We need more case studies and deeper breakdowns of claim categories.

**Find:**
- Detailed M&A failure case studies where post-close issues were traceable to information that existed in the data room but wasn't synthesized (not fraud/concealment — synthesis failures)
- RWI claims data broken down by specific type: financial statement breaches, tax issues, material contracts, environmental, IP, employment — which categories are growing fastest?
- What specifically constitutes a "financial statement breach" in RWI claims — revenue recognition? Off-balance-sheet liabilities? Inventory valuation? Intercompany transactions? Working capital adjustments?
- Case studies from litigation (Delaware Chancery Court, earnout disputes, indemnification claims) that reveal what DD should have caught
- Insurance industry analysis of DD quality — what do RWI underwriters look for that deal teams miss? What red flags do underwriters screen for?
- Research on "near misses" — deals where issues were caught just before close, and what triggered the discovery
- Data on the correlation between DD scope/thoroughness and post-close claim rates
- The role of cross-document connections in specific failures — cases where reviewing Document A alone and Document B alone wouldn't reveal the problem, but comparing them would

**Sources to prioritize:**
- SRS Acquiom annual claims reports (detailed breakdowns beyond headlines)
- Euclid Transactional claims studies
- AIG M&A Claims Intelligence Series
- RWI broker analyses (Woodruff Sawyer, CAC Specialty, Aon, Marsh)
- Delaware Court of Chancery opinions on M&A disputes
- Forensic accounting literature and case studies
- Harvard Law School Forum on Corporate Governance (M&A posts)
- M&A litigation analysis firms
- Ernst & Young, KPMG, Deloitte forensic practice publications

**For each finding, provide:**
- Claim: [the finding]
- Source: [author/org name]
- Date: [publication date]
- URL: [url where available]
- Quote: [verbatim excerpt if available]
- Relevance: [specific type of DD failure this illustrates]

**Do NOT research:** Market size, TAM/SAM/SOM, dollar figures for market opportunity, number of potential users, industry revenue, or market growth trends. Focus only on what goes wrong and why.

---

## Prompt 3

**Save as:** `legal-dd.md`

---

I'm building a problem thesis for Rubicon, an AI-powered due diligence platform for private equity, investment banking, and M&A professionals. This is Round 2 research — going deeper on specific gaps from an initial research round.

**The core thesis:** Due diligence fails because it requires cross-document synthesis of thousands of unstructured pages within deal-driven timelines that structurally exceed human cognitive capacity. Current tools (virtual data rooms) store documents securely but do nothing to make them analytically accessible.

**This prompt focuses on:** Legal due diligence workflows, contract review processes, and where legal DD specifically breaks down.

**What prior research already established (do not re-find these — go deeper):**
- Lawyers are one of 5 affected stakeholder groups in DD
- "M&A sucks. I hate diligence" (Glassdoor, Big Law senior associate)
- "Disclosure schedules may be the death of me" (Glassdoor, Big Law)
- Contract review tools (Kira/Litera, Luminance, Evisort) exist but have accuracy and usability problems
- Traditional DD examines only 5-10% of the contract population in large transactions (Sirion.ai, 2026)
- Kira accuracy "not good enough to trust 90+%" (G2 review); Luminance scored 1.5/5 usability (Nevada State Bar)

**The gap:** Legal DD is a massive part of the overall DD process and we have almost no hard data — just a few emotional quotes and tool reviews. We need specifics on workflows, time/effort, error rates, and failure modes.

**Find:**
- Time and effort data for legal due diligence: how many hours per contract? Per deal? What team sizes?
- Most common legal DD failures: what types of clauses, obligations, or risks are missed most often?
- Contract abstraction workflows: how do law firms currently extract and synthesize contract terms at scale? What is the manual process?
- Disclosure schedule management: how are schedules created, reviewed, and reconciled against the purchase agreement? What goes wrong?
- Change-of-control clause analysis: how often are these missed across hundreds of contracts and what are the consequences?
- Third-party consent requirements: how are these identified and tracked across hundreds of contracts? What happens when one is missed?
- The handoff between legal DD and financial/commercial DD: where do cross-workstream findings fall through?
- Law firm billing data or surveys on DD as a practice area — staffing, associate burnout, leverage ratios
- Legal tech adoption rates specifically for DD (not general legal tech adoption)
- IP due diligence workflows: patent review, trademark clearance, open source license analysis — how is this done and what's missed?
- Environmental and regulatory compliance review in DD: how thorough is it?
- The role of junior associates in legal DD: what work do they do, how are they supervised, where do errors enter?

**Sources to prioritize:**
- American Bar Association (ABA) surveys and publications
- Law firm knowledge management publications
- Legal tech industry reports (Legaltech News, ILTA surveys)
- ALM Intelligence / The American Lawyer (law firm economics)
- Practical Law (Thomson Reuters) — DD practice guides
- Big Law associate surveys (NALP, Above the Law)
- Contract management industry data (WorldCC/IACCM)
- Georgetown/Thomson Reuters law firm innovation reports
- M&A law firm blogs and practice guides (Wachtell, Skadden, Kirkland, Latham)

**For each finding, provide:**
- Claim: [the finding]
- Source: [author/org name]
- Date: [publication date]
- URL: [url where available]
- Quote: [verbatim excerpt if available]
- Relevance: [specific legal DD workflow or failure point this addresses]

**Do NOT research:** Market size, TAM/SAM/SOM, dollar figures for market opportunity, number of potential users, industry revenue, or market growth trends. Focus only on how legal DD works and where it fails.

---

## Prompt 4

**Save as:** `qa-bottleneck.md`

---

I'm building a problem thesis for Rubicon, an AI-powered due diligence platform for private equity, investment banking, and M&A professionals. This is Round 2 research — going deeper on specific gaps from an initial research round.

**The core thesis:** Due diligence fails because it requires cross-document synthesis of thousands of unstructured pages within deal-driven timelines that structurally exceed human cognitive capacity. Current tools (virtual data rooms) store documents securely but do nothing to make them analytically accessible.

**This prompt focuses on:** The Q&A bottleneck in M&A due diligence — the mechanics of how deal teams communicate questions, requests, and findings, and why this consumes up to 70% of deal time.

**What prior research already established (do not re-find these — go deeper):**
- Up to 70% of deal time is spent on Q&A — the most intense information retrieval stage (Ansarada, 2025)
- "It's definitely like herding cats. Upwards of 100 people, between advisors and internal folks" (Rob Humble, SVP Strategy, IAS)
- Multiple DD question lists arrive "in different formats and file types (excel and word)" (Wall Street Oasis)
- ~33% said stakeholder alignment delayed transactions (Deloitte, 2024)
- VDR Q&A modules are the primary coordination tool but are widely criticized

**The gap:** The Q&A phase is where most deal time is consumed, but we have almost no detail on WHY. We know it takes 70% of time but not the mechanics — volume, response latency, clarification loops, duplicates, routing failures.

**Find:**
- Q&A volume data: how many questions/requests are typical per M&A deal? By workstream? Has volume grown over time?
- Response time data: average time to respond to DD Q&A items. What causes the biggest delays?
- Duplicate question analysis: what percentage of Q&A items are substantively duplicative across different buyer workstreams asking the same target company?
- The Q&A workflow end-to-end: who formulates questions, who routes them, who answers, how are answers tracked as complete?
- Specific examples of Q&A coordination failures causing confusion, delays, or missed information
- How sell-side teams (target company management, sell-side advisors) manage inbound Q&A from multiple buyer workstreams
- VDR Q&A module capabilities and limitations: what do Datasite, Intralinks, Ansarada, and Ideals Q&A features actually do? Where do they fall short?
- Expert network calls and management presentations: how do verbal/interview-based DD findings get integrated with document-based findings? Do they?
- The "bring-down" and "closing set" coordination: how do teams manage the final push to close?
- How many rounds of follow-up questions are typical? What triggers escalation?
- The role of the "diligence tracker" or "request list" spreadsheet that most deals use alongside the VDR

**Sources to prioritize:**
- VDR vendor whitepapers and platform data (Datasite, Ansarada, Intralinks publish deal analytics)
- M&A advisory firm publications and process guides
- Investment banking training materials and analyst guides
- Sell-side M&A process guides (target company perspective)
- Corporate development blogs and podcasts
- Wall Street Oasis, Mergers & Inquisitions (practitioner accounts)
- M&A process management tool reviews and comparisons

**For each finding, provide:**
- Claim: [the finding]
- Source: [author/org name]
- Date: [publication date]
- URL: [url where available]
- Quote: [verbatim excerpt if available]
- Relevance: [specific Q&A or coordination pain point this addresses]

**Do NOT research:** Market size, TAM/SAM/SOM, dollar figures for market opportunity, number of potential users, industry revenue, or market growth trends. Focus only on how Q&A works and where it breaks.

---

## Prompt 5

**Save as:** `portfolio-monitoring.md`

---

I'm building a problem thesis for Rubicon, an AI-powered due diligence platform for private equity, investment banking, and M&A professionals. This is Round 2 research — going deeper on specific gaps from an initial research round.

**The core thesis:** Due diligence fails because it requires cross-document synthesis of thousands of unstructured pages within deal-driven timelines that structurally exceed human cognitive capacity. Current tools (virtual data rooms) store documents securely but do nothing to make them analytically accessible.

**This prompt focuses on:** Portfolio monitoring failures in private equity — what happens when GPs can't see what's happening in their portfolio companies between formal reporting periods, and the real-world consequences.

**What prior research already established (do not re-find these — go deeper):**
- 46% of private capital fund managers rely exclusively on Excel for portfolio monitoring (Preqin, 2017)
- 54% of portfolio companies exchange data via email with attachments; 36% text-only email (PwC, 2022)
- Portfolio data in PMS is often 30-45 days old by the time it is reconciled (V7 Labs, 2025)
- ILPA's 60-day quarterly reporting standard has ~50% adoption rate
- PE exit backlog of 18,000+ companies held beyond traditional 4-year horizon; 6.7-year average hold (McKinsey, 2025)
- "Investors are flying blind at precisely the time when vigilance matters most" (MSCI Research)
- Monitoring time declined from 29% to 19% of operating group time (McKinsey, 2018)
- 80% of investors expect higher transparency from fund managers (Intertrust Group)

**The gap:** We have good stats on HOW monitoring is broken (Excel, email, stale data) but weak evidence on WHAT HAPPENS when it fails. Need case studies of consequences, the actual data flow mechanics, and how extended hold periods compound the problem.

**Find:**
- Case studies of portfolio company failures (operational, financial, compliance) where earlier detection by the GP could have changed outcomes
- The actual data flow: how does information move from portfolio company CFO → fund operating partner → investment committee → LPs? Where does it break down at each step?
- What specific metrics do GPs actually track? How many KPIs per portco? How many tracked manually vs. systematically?
- LP frustrations with GP reporting: what do LPs want that they're not getting? How does this affect fundraising for next fund?
- The cost of "fighting fires" — how much GP operating partner time goes to crisis management vs. proactive value creation?
- How has the extended hold period (4 years → 6.7 years) changed what monitoring must cover?
- ESG monitoring requirements: how are these new requirements being handled (or not) and what gaps exist?
- Board reporting vs. fund reporting disconnects: does the GP board deck for a portco match what's in the LP quarterly report?
- Add-on acquisition monitoring: when a platform company does bolt-on acquisitions, how is the combined entity monitored? Does integration fall through the cracks?
- Real examples of PE firms discovering portco problems too late — revenue deterioration, compliance failures, management issues that should have been caught earlier
- How do GPs handle the "denominator problem" — more companies to monitor with the same (or shrinking) operating team?

**Sources to prioritize:**
- PE industry publications (PitchBook, Preqin, Bain PE annual reports, McKinsey Global Private Markets)
- LP association publications (ILPA best practices, institutional investor forums)
- Fund administration industry data and publications
- Operating partner community content (Operating Partner Forum, Value Creation Network, PE Hub)
- PE-focused consulting publications (McKinsey, Bain, BCG PE practices)
- Portfolio monitoring software vendors (Allvue, Cobalt, eFront/BlackRock, Chronograph) — their case studies and whitepapers
- GP-LP relationship research and surveys

**For each finding, provide:**
- Claim: [the finding]
- Source: [author/org name]
- Date: [publication date]
- URL: [url where available]
- Quote: [verbatim excerpt if available]
- Relevance: [specific monitoring failure or consequence this illustrates]

**Do NOT research:** Market size, TAM/SAM/SOM, dollar figures for market opportunity, number of potential users, industry revenue, or market growth trends. Focus only on how monitoring fails and the consequences.

---

## Prompt 6

**Save as:** `ai-adoption-barriers.md`

---

I'm building a problem thesis for Rubicon, an AI-powered due diligence platform for private equity, investment banking, and M&A professionals. This is Round 2 research — going deeper on specific gaps from an initial research round.

**The core thesis:** Due diligence fails because it requires cross-document synthesis of thousands of unstructured pages within deal-driven timelines that structurally exceed human cognitive capacity. Current tools (virtual data rooms) store documents securely but do nothing to make them analytically accessible.

**This prompt focuses on:** Why AI adoption in due diligence is so low — the specific barriers preventing deal teams from using AI tools for DD, even as AI adoption grows in other M&A workflows.

**What prior research already established (do not re-find these — go deeper):**
- Only 16% of M&A professionals currently use generative AI in due diligence (IBCA, 2024)
- 86% have integrated GenAI into M&A workflows broadly, but only 35% use it for DD specifically (Deloitte, 2025)
- 65% cite data quality and availability as a leading DD barrier (Deloitte, 2025)
- Only 3% of small/mid-sized firms actively use AI for DD vs. 29% of large firms
- AI tools that exist have accuracy and usability problems: Kira "not good enough to trust 90+%" (G2); Luminance 1.5/5 usability (Nevada State Bar); Evisort "cannot span interrelated contracts"
- "It looks amazing in the sales demo but struggles with the messy reality of their own documents" (Luminance user)
- AI adoption for M&A overall doubled to 45% in 2025 (Bain)

**The gap:** We know adoption is low but not specifically WHY. Is it technology limitations, security concerns, accuracy thresholds, workflow integration problems, regulatory barriers, cultural resistance, or all of the above? We need rank-ordered barriers and implementation failure stories.

**Find:**
- Survey data on specific barriers to AI adoption in DD: rank-ordered reasons firms DON'T use AI for due diligence
- Security and confidentiality concerns: how do deal teams evaluate AI tool security? What standards must tools meet? Is the concern about data leaving the environment, model training on deal data, or something else?
- Accuracy thresholds: what error rate is acceptable in DD context? How does this compare to what current AI tools deliver? What is "good enough" for a lawyer or analyst to trust?
- Regulatory and liability barriers: are there legal or regulatory constraints on using AI for DD? What are the professional liability implications if AI misses something?
- Cultural resistance: partner and managing director attitudes toward AI in advisory work. Is "trust" the real barrier? Generational divide?
- Implementation failure stories: firms that tried AI for DD and rolled back or abandoned the effort. What specifically went wrong?
- The "demo vs. reality" gap: documented cases of AI tools performing differently on actual deal documents vs. curated demo data
- Workflow integration challenges: how do (or don't) AI tools fit into existing DD processes? What would have to change in a firm's workflow to use AI effectively?
- Data security requirements for DD (encryption, access controls, SOC 2, data residency) and how current AI tools meet or fail to meet them
- The firm size gap: why do large firms adopt 10x more than small/mid firms? Is it budget, deal volume, risk tolerance, dedicated innovation teams?
- What would make deal professionals WILLING to use AI? What would tip the balance?

**Sources to prioritize:**
- Management consulting surveys on AI in M&A (Deloitte, PwC, EY, KPMG annual reports)
- Legal tech adoption surveys (ILTA, ABA TechReport, Legal Evolution)
- AI vendor case studies — both successes and failures
- PE and IB industry AI adoption reports
- Information security standards for financial services (SOC 2, ISO 27001 in deal context)
- Professional liability and malpractice publications
- CIO/CTO surveys from PE firms and law firms
- Alternative Legal Service Provider (ALSP) industry data
- Law firm innovation officer interviews and publications

**For each finding, provide:**
- Claim: [the finding]
- Source: [author/org name]
- Date: [publication date]
- URL: [url where available]
- Quote: [verbatim excerpt if available]
- Relevance: [specific adoption barrier or enabler this identifies]

**Do NOT research:** Market size, TAM/SAM/SOM, dollar figures for market opportunity, number of potential users, industry revenue, or market growth trends. Focus only on adoption barriers and what would change them.

---

## Prompt 7

**Save as:** `sell-side-burden.md`

---

I'm building a problem thesis for Rubicon, an AI-powered due diligence platform for private equity, investment banking, and M&A professionals. This is Round 2 research — going deeper on specific gaps from an initial research round.

**The core thesis:** Due diligence fails because it requires cross-document synthesis of thousands of unstructured pages within deal-driven timelines that structurally exceed human cognitive capacity. Current tools (virtual data rooms) store documents securely but do nothing to make them analytically accessible.

**This prompt focuses on:** The sell-side DD burden — specifically on target company management teams going through M&A and the sell-side advisors managing the process. This is distinct from startup founders raising venture capital.

**What prior research already established (do not re-find these — go deeper):**
- "Expect this to be a full-time job for one of the founders or executives for months" (Cremades)
- "Nearly all founders hold themselves together well during the process only to break during the due diligence" (M&A advisor, 20 years experience)
- Target company leadership pain: diligence disrupts daily operations, duplicate requests cause confusion and fatigue, concern over misinterpreted or missing data
- 46% of founders spend 30%+ of their week on fundraising (Angel Investment Network, 2025)
- Average successful seed raise: 12.5 weeks, 58 investors, 40 meetings (DocSend/HBS)

**The gap:** Round 1 conflated two distinct groups: (1) startup founders raising venture capital and (2) established company management teams going through M&A sale processes. These are very different experiences. We need specific data on the M&A sell-side burden — the management team of a company being acquired, and the sell-side investment bankers and advisors managing the process.

**Find:**
- Operational impact on M&A target companies: how many management person-hours go to DD? Which C-suite and functional roles are most affected (CEO, CFO, Controller, HR head, IT)?
- Sell-side advisor workflows: how do investment banks and M&A advisors manage the DD process for their sell-side clients? What is their role in fielding buyer questions?
- Data room preparation: how long does it take to build a sell-side data room? Who does the work? What goes wrong? How many documents typically?
- Vendor due diligence (VDD) trends: are sell-side teams increasingly running their own pre-emptive DD before going to market? How does this change the buyer's DD process?
- Confidentiality management during DD: how do target companies maintain employee, customer, and supplier confidentiality while providing detailed information to potential buyers?
- Management presentation preparation: how much executive time does this consume? How many presentations per deal process?
- "Information request fatigue": how many overlapping requests does a target company receive from different buyer workstreams (financial, legal, tax, IT, HR, commercial, environmental)?
- Impact on business performance during DD: does the target company's business measurably suffer while management is distracted by the sale process?
- Failed deal aftermath: what happens to the target company and its management team after a deal falls through during DD? How often does this happen?
- The two-sided frustration: evidence for buyers thinking sellers are withholding information AND sellers thinking buyers are being unreasonable or duplicative
- Broken deal costs for the sell-side: what has the target company invested in a process that doesn't close?

**Sources to prioritize:**
- Sell-side M&A advisory guides (middle market investment banks, boutique advisory firms)
- Corporate development officer surveys and publications
- M&A process guides from law firms (sell-side perspective specifically)
- Business owner and founder forums (Exit Planning Exchange, EPI, Axial)
- Middle Market Alliance / ACG (Association for Corporate Growth) publications
- Vendor due diligence (VDD) trend reports
- Sell-side data room management guides and best practices
- Post-deal retrospective surveys
- M&A advisor podcasts and interviews (Colonnade Advisors, Axial Forum, etc.)

**For each finding, provide:**
- Claim: [the finding]
- Source: [author/org name]
- Date: [publication date]
- URL: [url where available]
- Quote: [verbatim excerpt if available]
- Relevance: [specific sell-side burden or pain point this addresses]

**Do NOT research:** Market size, TAM/SAM/SOM, dollar figures for market opportunity, number of potential users, industry revenue, or market growth trends. Focus only on the sell-side experience and burden.

---

## Prompt 8

**Save as:** `cross-workstream-synthesis.md`

---

I'm building a problem thesis for Rubicon, an AI-powered due diligence platform for private equity, investment banking, and M&A professionals. This is Round 2 research — going deeper on specific gaps from an initial research round.

**The core thesis:** Due diligence fails because it requires cross-document synthesis of thousands of unstructured pages within deal-driven timelines that structurally exceed human cognitive capacity. Current tools (virtual data rooms) store documents securely but do nothing to make them analytically accessible.

**This prompt focuses on:** Cross-workstream synthesis failures — how the parallel structure of DD (8-10+ independent workstreams) creates structural blind spots, and why connecting findings across teams is the hardest and most failure-prone part of due diligence.

**What prior research already established (do not re-find these — go deeper):**
- Standard DD involves 8-10+ parallel workstreams: finance, legal, tax, IT, HR, commercial, operations, environmental, insurance, integration (Altrata/Deloitte, 2024-2025)
- Acquirers using siloed advisers were nearly 2x more dissatisfied with ability to synthesize findings (Deloitte, 2023)
- The most inaccurate DD areas are integration roadmaps, revenue synergies, and people issues — all requiring cross-team coordination (Bain, 2023)
- HP/Autonomy: 7 advisory firms with documented communication breakdown
- "Each diligence team is only keyed in on one aspect of the deal. Need people who have a better view of everything" (Wall Street Oasis)
- Our processing log explicitly noted: "quantitative data on duplicated effort across workstreams is scarce"

**The gap:** This is the structural heart of the problem but our evidence is mostly anecdotal — a few quotes and one case study. We need specifics on how workstreams communicate (or fail to), what types of cross-workstream connections are most critical and most often missed, and what integration methods have been tried.

**Find:**
- How DD workstreams actually communicate findings to each other: what tools, meetings, reporting structures, and handoff processes do deal teams use?
- Specific examples of cross-workstream gaps causing deal problems: financial findings that contradicted legal findings, HR issues that affected commercial projections, IT vulnerabilities that created compliance risk, environmental issues that affected valuation
- The role of the "deal captain," project manager, or integration lead: who is responsible for synthesis across workstreams? How well does this role work in practice?
- Integrated vs. siloed advisory models: is there comparative data on outcomes when using a single advisory firm (like Big 4 providing multiple DD services) vs. multiple specialist firms?
- Deal management platform adoption and effectiveness: DealCloud, Midaxo, 4Degrees, Ansarada Deals — do these help with cross-workstream synthesis or just task tracking?
- The "synthesis meeting" or "red flag session": how often do deal teams formally convene to connect findings across workstreams? How effective is it? What format does it take?
- Quantitative data on duplicated effort across workstreams: how much review work is redundant because different teams examine the same documents independently?
- Cross-workstream risk types that are systematically underanalyzed: working capital vs. revenue quality vs. contract risk interactions; tax structure vs. legal entity vs. financial reporting alignment
- How the final investment committee memo is created: who writes it? How are findings from 8-10 workstreams synthesized into a go/no-go recommendation? What gets lost in translation?
- Analogies from other domains where cross-functional synthesis failures have catastrophic consequences: intelligence analysis (9/11 Commission "connecting the dots"), medical team coordination (surgical team failures), engineering (NASA Columbia disaster — silo failures)
- Academic or consulting research on organizational synthesis failures in professional services contexts

**Sources to prioritize:**
- M&A integration and process management research
- Professional services firm publications on DD methodology
- Deal management software vendor research and case studies
- Organizational behavior research on cross-functional team failures
- Investment committee decision-making studies
- Post-mortem analyses of M&A failures attributed to siloed DD
- Advisory firm methodology guides (Big 4 transaction advisory, strategy consulting DD practices)
- Intelligence analysis literature on "connecting the dots" (9/11 Commission Report, RAND Corporation)
- Medical team coordination and patient safety research (closest professional analogy)
- Systems engineering and safety research (Perrow, Reason — organizational accident theory)

**For each finding, provide:**
- Claim: [the finding]
- Source: [author/org name]
- Date: [publication date]
- URL: [url where available]
- Quote: [verbatim excerpt if available]
- Relevance: [specific synthesis failure or coordination mechanism this illustrates]

**Do NOT research:** Market size, TAM/SAM/SOM, dollar figures for market opportunity, number of potential users, industry revenue, or market growth trends. Focus only on how cross-workstream synthesis fails and what has been tried to fix it.
