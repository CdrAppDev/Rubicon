# Due diligence pain points: what the evidence actually shows

**All six hypotheses find meaningful support in the evidence, though the strength and specificity of that support varies considerably.** The strongest evidence backs H1 (document overload under time pressure), H2 (missed anomalies surfacing post-close), and H6 (founder time drain during fundraising). H3, H4, and H5 are well-supported qualitatively but lack the rigorous time-study data needed for precise quantification. Across every hypothesis, a recurring deeper pattern emerges: the fundamental mismatch between the volume of unstructured information that must be processed and the cognitive bandwidth available to process it under deal-driven timelines.

What follows is a structured evidence assessment for each hypothesis, with findings in citation format and an honest appraisal of what the data does and does not tell us.

---

## H1: Thousands of unstructured documents under impossible timelines

This hypothesis receives the **strongest quantitative support** of all six. Multiple independent sources converge on the same picture: mid-market data rooms contain **5,000 to 50,000+ pages**, with large transactions reaching 75,000–100,000 pages. A typical M&A legal team spends six weeks manually reviewing approximately 50,000 pages of contracts, emails, and financial documents. Standard due diligence periods last **30 to 60 days**, with complex deals extending to 90+ days — yet the evidence shows these timelines are compressing relative to the work required.

- Claim: Most mid-market data rooms contain 5,000–50,000+ pages depending on company size and complexity
- Relevant hypothesis: H1
- Source: Investor Data Rooms
- Date: 2025
- URL: https://investordatarooms.com/virtual-data-rooms-for-ma-deals/
- Quote: "5,000-50,000+ pages of documents, depending on the size and complexity of the company."

- Claim: A typical M&A transaction involves a legal team spending six weeks manually reviewing 50,000 pages
- Relevant hypothesis: H1
- Source: Sirion
- Date: January 9, 2026
- URL: https://www.sirion.ai/library/contract-ai/ai-due-diligence/
- Quote: "Imagine a typical M&A transaction: a legal team spends six weeks manually reviewing 50,000 pages of contracts, emails, and financial documents."
- ⚠️ Uncertainty: Software vendor scenario, not a rigorous statistical finding — but consistent with other sources.

- Claim: On average, DD takes a team of 10 around 6 weeks; complex deals extend to 90+ days
- Relevant hypothesis: H1
- Source: Eton Venture Services
- Date: 2025
- URL: https://etonvs.com/transaction-valuation-advisory/financial-due-diligence/
- Quote: "On average, it takes a team of 10 around 6 weeks. In more complex deals, due diligence can extend to 90 days or longer."

- Claim: Average deal time on Datasite increased from **6.9 months in 2021 to 10.2 months in 2024** — a 48% increase attributed to growing DD complexity
- Relevant hypothesis: H1
- Source: Datasite (anonymized platform data)
- Date: May 20, 2025
- URL: https://www.datasite.com/en/resources/insights/expert-spotlight-strategies-for-due-diligence-and-deal-success
- Quote: "According to anonymized data from Datasite, the average time a deal remained on its platform increased from 6.9 months in 2021 to 10.2 months in 2024. Much of this elongation is attributed to the growing complexity and depth of the due diligence process."

- Claim: Among deal professionals experiencing extended DD timelines, **59% say 1–3 months have been added** to the process; 45% identified technology review as the most arduous aspect
- Relevant hypothesis: H1
- Source: SRS Acquiom & Mergermarket (survey of 150 senior executives at U.S. investment banks, Q3 2024)
- Date: 2025
- URL: https://www.srsacquiom.com/our-insights/m-a-due-diligence-study/
- Quote: "Among those experiencing extended due diligence timelines, 59% say that 1–3 months have been added to the process." / "45% of participants in this study called out technology review as the most expensive and arduous aspect of M&A due diligence."

- Claim: The average time spent on due diligence for technology companies is 12 weeks
- Relevant hypothesis: H1
- Source: KPMG (as cited by Beyond M&A)
- Date: 2023
- URL: https://beyond-ma.com/10-must-know-statistics-about-tech-due-diligence/
- Quote: "The average time spent on due diligence for technology companies is 12 weeks. (Source: KPMG)"
- ⚠️ Uncertainty: Secondary citation; original KPMG report not directly accessed.

- Claim: Deals that closed in 2021 and early 2022 experienced conditions where "thorough due diligence was impractical"
- Relevant hypothesis: H1
- Source: SRS Acquiom
- Date: 2025
- URL: https://www.srsacquiom.com/our-insights/ma-claims-undisclosed-liabilities-earnouts/
- Quote: "This contrasts with deals that closed in 2021 and early 2022, when valuations and the pace of dealmaking made such thorough due diligence impractical."

- Claim: Even small business DD requires "hundreds of hours" with help from accounting and legal professionals
- Relevant hypothesis: H1
- Source: Stimmel Law
- Date: Undated
- URL: https://www.stimmel-law.com/en/articles/due-diligence-what-it-and-how-you-do-it
- Quote: "Due diligence is a truly major effort and the average transaction, even if dealing with relatively small businesses, requires hundreds of hours of due diligence."

**Assessment:** H1 is **strongly supported**. The derived person-hour estimate for a mid-market deal — a team of 10 working 6 weeks — yields roughly **2,400 person-hours** at standard workweeks, and likely more given deal-intensity schedules. The Datasite platform data showing a 48% increase in deal duration from 2021 to 2024 is particularly compelling because it comes from anonymized usage data across thousands of deals rather than self-reported surveys. SRS Acquiom's finding that "thorough due diligence was impractical" during the 2021 boom directly validates the hypothesis language. This is a **root problem**, not a symptom — the volume of unstructured information fundamentally exceeds the processing capacity available within deal timelines.

---

## H2: Financial anomalies go undetected because cross-referencing is impractical at scale

The evidence strongly supports this hypothesis, with particularly striking data from representation and warranty insurance claims and two landmark case studies that illustrate cross-referencing failures with surgical precision.

- Claim: Post-closing indemnification claims occur in **~30% of all M&A deals**
- Relevant hypothesis: H2
- Source: SRS Acquiom
- Date: 2022
- URL: https://www.srsacquiom.com/our-insights/post-closing-indemnification/
- Quote: "Post-closing indemnification claims activity...decreased to 30% of all deals experiencing at least one claim, compared to 38% of deals in the previous SRS Acquiom M&A Claims Insights Report released in 2020."

- Claim: Roughly **20% of RWI policies** generate claims within three years; financial statement breaches are the #1 category (20% of claims, **over 35% of total insurer payouts**)
- Relevant hypothesis: H2
- Source: Norton Rose Fulbright / Aon
- Date: February 2022
- URL: https://www.projectfinance.law/publications/2022/february/representations-and-warranties-insurance-data-and-claims/
- Quote: "Financial statement breaches are the most commonly alleged breach of a representation and warranty, having been cited in almost 20% of all claims...accounting for over 35% of the total amount paid by insurers despite making up only 20% of claims."

- Claim: **44% of RWI claims are now reported after 18 months post-close** — a growing trend
- Relevant hypothesis: H2
- Source: Euclid Transactional
- Date: 2023
- URL: https://sspins.com/did-you-know-series-2023-rwi-claims-reports-and-analysis/
- Quote: "Later reporting is a growing trend, with 44% of claims now being reported after 18 months post-close."

- Claim: Claims for breach of financial statement representations on **audited** target companies are received at a higher rate than on unaudited targets
- Relevant hypothesis: H2
- Source: Euclid Transactional
- Date: 2023
- URL: https://sspins.com/did-you-know-series-2023-rwi-claims-reports-and-analysis/
- Quote: "Though counterintuitive, claims for breach of Financial Statement reps on audited target companies are received at a higher rate than unaudited target companies."

- Claim: More than **60% of senior executives** cited poor due diligence as the main reason M&A failed to deliver expected value
- Relevant hypothesis: H2
- Source: Bain & Company (2020 Global Corporate M&A Report)
- Date: 2020
- URL: https://aliasintelligence.com/due-diligence-process-essential-steps-for-high-stakes-deals/
- Quote: "According to Bain's 2020 Global Corporate M&A Report, more than 60% of senior executives cited poor due diligence as the main reason mergers and acquisitions failed to deliver expected value."

- Claim: HP's CFO never read the preliminary DD report; KPMG's draft DD report was the only external DD HP commissioned on Autonomy. Autonomy's revenue was inflated 12-15% annually through round-trip transactions
- Relevant hypothesis: H2
- Source: AccountingWEB / The Register / CorpDev.org
- Date: 2019-2025
- URL: https://www.accountingweb.co.uk/business/finance-strategy/hps-cfo-admits-to-never-reading-the-due-diligence-on-autonomy
- Quote: "Not only did she never read the preliminary due diligence, no further due diligence whatsoever went ahead after the report."

- Claim: Caterpillar discovered "deliberate, multi-year, coordinated accounting misconduct" at Siwei only after closing, when a physical inventory count revealed discrepancies with accounting records — despite Deloitte and Ernst & Young conducting DD
- Relevant hypothesis: H2
- Source: Caterpillar press release / M&A Science / Mining.com
- Date: January 2013
- URL: https://medium.com/m-a-science/how-the-lack-of-due-diligence-destroys-deals-3-examples-6d81348b270d
- Quote: "A few months after the transaction had closed, a member of Caterpillar's financial team noticed discrepancies between the inventories listed on Siwei's balance sheet and its physical inventory. This, in turn, led to a trail of fraudulent documents."

- Claim: Claims for breach of the "No Undisclosed Liabilities" representation are surging
- Relevant hypothesis: H2
- Source: SRS Acquiom
- Date: 2025
- URL: https://www.srsacquiom.com/our-insights/ma-claims-undisclosed-liabilities-earnouts/
- Quote: "Claims for breach of the No Undisclosed Liabilities representation are way up."

**Assessment:** H2 is **strongly supported**. The RWI claims data is the most objective evidence available — it represents real money being paid on real missed risks. That **30% of deals** generate post-close indemnification claims and **44% of those claims surface only after 18 months** means the DD process systematically fails to catch a meaningful fraction of material issues. The Caterpillar/Siwei case is a textbook cross-referencing failure: a basic reconciliation of physical inventory against accounting records would have caught the fraud during DD, but two separate advisory firms working on different aspects apparently never performed this check. The counterintuitive finding that audited companies generate *more* financial statement breach claims than unaudited ones suggests that audit opinions may create false comfort that reduces DD scrutiny — a subtle but important nuance.

However, H2 is partly a **symptom** of deeper causes. The root problems are: (1) the sheer volume that makes systematic cross-referencing impractical manually (H1), (2) the fragmented team structure that prevents information from being synthesized across workstreams (H4), and (3) time pressure that forces heuristic rather than analytical processing.

---

## H3: Search time dominates review time, causing incomplete analysis

- Claim: The average diligence analyst spends roughly **37% of their time** hunting through emails, files, and spreadsheets
- Relevant hypothesis: H3
- Source: DiligenceVault
- Date: 2025
- URL: https://diligencevault.com/transforming-due-diligence-and-reporting-with-diligencevault/
- Quote: "Did you know that the average diligence analyst spends roughly 37% of their time hunting through emails, files, and spreadsheets?"
- ⚠️ Uncertainty: Software vendor marketing; underlying study source not cited. Directionally consistent with other sources.

- Claim: As much as **70% of deal time** can be spent on Q&A — the most intense stage where finding specific information across documents is critical
- Relevant hypothesis: H3
- Source: Ansarada
- Date: 2025
- URL: https://www.ansarada.com/due-diligence/process
- Quote: "As much as 70% of deal time can be spent on Q&A."

- Claim: AI-powered Smart Review locates 95% of relevant information in just **10% of total documentation** — implying 90% of manually reviewed documents contain only 5% of relevant information
- Relevant hypothesis: H3
- Source: Imprima (via Kumo)
- Date: 2025
- URL: https://www.withkumo.com/blog/top-5-due-diligence-tools-for-ma-professionals
- Quote: "The Smart Review function helps teams locate 95% of relevant information while reviewing just 10% of the total documentation."

- Claim: Among boutique investment banks, **40% identified incomplete information** on a target as one of the greatest DD hurdles
- Relevant hypothesis: H3
- Source: SRS Acquiom & Mergermarket (2024 survey)
- Date: 2025
- URL: https://www.srsacquiom.com/our-insights/m-a-due-diligence-study/
- Quote: "Among boutique investment banks, 40% of respondents identified incomplete information on a target company as one of the greatest due diligence hurdles."

- Claim: Before AI, analysts spent 90% of their time on data processing and only 10% on strategic judgment
- Relevant hypothesis: H3
- Source: Copia Wealth Studios (citing Soal Lab co-founder)
- Date: September 10, 2025
- URL: https://copiawealthstudios.com/blog/why-ai-powered-due-diligence-is-the-new-normal-in-private-equity
- Quote: "Before AI, analysts spent 90% of their time crunching numbers and only 10% on strategic judgment."
- ⚠️ Uncertainty: AI vendor blog; the 90/10 ratio may be directionally correct but imprecise.

- Claim: The outside-in diligence process "used to require weeks of manual effort" for sourcing data, mining data rooms, and stitching together insights
- Relevant hypothesis: H3
- Source: McKinsey & Company
- Date: 2025
- URL: https://www.mckinsey.com/capabilities/transformation/our-insights/from-potential-to-performance-using-gen-ai-to-conduct-outside-in-diligence
- Quote: "The outside-in diligence process used to require weeks of manual effort from a diligence team—sourcing public data, mining the seller's data room, scraping external signals, triangulating expert input, and stitching together all those insights."

**Assessment:** H3 is **supported, with caveats about data precision**. The 37% search-time figure and 90/10 processing-vs.-judgment ratio both originate from software vendors with commercial interests, so exact figures should be treated as approximate. However, multiple independent sources converge on the same directional finding: the majority of analyst time goes to finding and processing information rather than analyzing it. The Imprima statistic is particularly revealing — if 95% of relevant information is concentrated in just 10% of documentation, manual reviewers face a needle-in-a-haystack problem at scale. **No rigorous, independent academic time-motion study of DD analyst workflows was found** — this is a notable gap in the evidence base.

H3 is closely intertwined with H1. The search problem is a *consequence* of the volume problem. If data rooms contained 50 well-organized pages instead of 50,000 unstructured ones, search time would be trivial. The root cause is the combination of volume, lack of structure, and time pressure.

---

## H4: Fragmented collaboration yields inconsistent assessments and duplicated work

- Claim: Standard DD involves multi-workstream analysis spanning **8-10+ parallel tracks**: finance, legal, tax, IT, HR, commercial, operations, environmental, insurance, and integration — each typically assigned to separate teams or third-party advisors
- Relevant hypothesis: H4
- Source: Altrata / Deloitte
- Date: 2024-2025
- URL: https://altrata.com/articles/merger-due-diligence-checklist
- Quote: "Confirmatory deep-dive diligence: Multi-workstream analysis spanning finance, legal, tax, IT, HR, commercial, and operations."

- Claim: Without deliberate structure, DD workstreams operate in silos; teams don't systematically challenge each other's findings
- Relevant hypothesis: H4
- Source: Dealert.ai
- Date: 2025
- URL: https://dealert.ai/blog/p/due-diligence-software-the-digital-backbone-of-modern-deal-evaluation-and-risk-management/
- Quote: "Without structure, workstreams drift. The commercial team interviews customers, the ops team runs site visits, and the finance team reconciles numbers, yet no one systematically challenges each other's findings."

- Claim: Deal teams become so focused on individual workplan sections that they miss the overall risk picture
- Relevant hypothesis: H4
- Source: Midaxo
- Date: Undated
- URL: https://www.midaxo.com/blog/10-steps-to-a-more-successful-ma-due-diligence-process
- Quote: "The due diligence team can get so focused on reviewing their individual sections of the workplan that they miss the 'big picture'."

- Claim: ESG due diligence specifically suffers from "duplicative workstreams" and disconnect between DD findings and deal team needs
- Relevant hypothesis: H4
- Source: Novata
- Date: 2025
- URL: https://www.novata.com/resources/podcasts/smarter-esg-due-diligence-for-private-markets/
- Quote: "Common bottlenecks that you see from that process include duplicative workstreams, a mismatch between report outputs and the level of concision needed by deal teams."

- Claim: HP's DD involved at least **7 advisory firms**, yet there was a documented breakdown in communications between HP's auditors
- Relevant hypothesis: H4
- Source: Computer Weekly
- Date: 2015
- URL: https://www.computerweekly.com/news/4500254380/Legal-documents-reveal-HPs-failings-over-Autonomy
- Quote: "The filings highlight a breakdown in communications between HP's auditors, putting into question whether HP had conducted thorough due diligence exercise on the acquisition."

- Claim: After the Autonomy scandal, HPE implemented Risk Management Committees specifically to ensure "alignment between financial, legal, and operational teams"
- Relevant hypothesis: H4
- Source: Ainvest
- Date: 2025
- URL: https://www.ainvest.com/news/scandal-reform-hpe-autonomy-debacle-lessons-governance-2507/
- Quote: "Risk Management Committees: A senior executive-led committee to oversee M&A risks, ensuring alignment between financial, legal, and operational teams."

- Claim: The most inaccurate areas of DD are integration roadmaps, revenue synergies, and people issues — all of which require cross-team coordination
- Relevant hypothesis: H4
- Source: Bain & Company
- Date: 2023
- URL: https://www.bain.com/insights/due-diligence-m-and-a-report-2023/
- Quote: "The most inaccurate areas of diligence are integration roadmaps, revenue synergies, and people issues."

**Assessment:** H4 is **supported qualitatively but lacks precise quantitative evidence** on time wasted through duplication. No study was found that directly measures the hours lost to duplicated review effort across workstreams. The structural conditions for fragmentation are well-documented (8-10+ parallel workstreams, separate advisory firms, siloed tools), and the HP/Autonomy case provides a dramatic real-world example of what happens when cross-team synthesis fails. The fact that HPE's post-scandal reforms specifically targeted cross-team alignment confirms that fragmentation was a recognized failure mode.

H4 is both a **root problem and a symptom**. The root problem is that DD is structured as parallel, independent workstreams by design — each specialist team focuses on their domain. The fragmentation is an emergent property of this structure, exacerbated by tight timelines that leave no room for cross-workstream synthesis meetings. Bain's finding that the *least accurate* DD areas are precisely those requiring cross-team coordination (integration roadmaps, revenue synergies, people issues) powerfully illustrates this dynamic.

---

## H5: Portfolio monitoring blind spots between formal reporting periods

- Claim: PE fund managers typically report quarterly, but reports are **delayed by 1-2 quarters**; ILPA standards give 60 days after quarter-end for draft financials
- Relevant hypothesis: H5
- Source: Allvue Systems / ILPA
- Date: Current
- URL: https://www.allvuesystems.com/resources/private-equity-reporting-101-download-sample-private-equity-reports/
- Quote: "Most private equity managers value and report on their fund performance and portfolios on a quarterly basis, though delayed by one to two quarters."

- Claim: **54% of PE portfolio companies** use email with attachments to collect and share data; 61% build reports or decks manually; 36% respond via text-only email
- Relevant hypothesis: H5
- Source: PwC "Next In Private Equity" Survey
- Date: ~2020-2022
- URL: https://www.pwc.com/us/en/industries/financial-services/library/private-equity-data-analytics.html
- Quote: "Fifty-four percent of PE portco respondents in our Next In survey use an email with an attachment to collect data and respond to requests...Sixty-one percent of portco respondents noted they build a report or a deck to report data to the PE firm."

- Claim: PE operating groups reduced time allocated to "monitoring and reporting" from **29% in 2015 to just 19% in 2018**, while "driving measurable performance improvement" increased from 40% to 49%
- Relevant hypothesis: H5
- Source: McKinsey & Company (survey of 45 PE firms)
- Date: 2018
- URL: https://www.mckinsey.com/industries/private-capital/our-insights/private-equity-operating-groups-and-the-pursuit-of-portfolio-alpha
- Quote: "In 2015, operating groups spent 29 percent of their time focused on 'monitoring and reporting' portfolio company performance, compared with 19 percent in 2018."

- Claim: The PE exit backlog has reached **18,000+ companies** held beyond the traditional 4-year horizon, extending average hold times to **6.7 years** vs. the 5.7-year norm — straining monitoring capacity
- Relevant hypothesis: H5
- Source: McKinsey Global Private Markets Report 2025
- Date: February 2025
- URL: https://neoform.partners/mckinsey-global-private-markets-report-2025/

- Claim: VCs spend an average of **18 hours per week** working with portfolio companies and **22 hours per week** on networking/sourcing, out of a 55-hour workweek
- Relevant hypothesis: H5
- Source: Gompers, Gornall, Kaplan & Strebulaev, Journal of Financial Economics (survey of 885 VCs)
- Date: January 2020
- URL: https://corpgov.law.harvard.edu/2019/08/20/how-do-venture-capitalists-make-decisions/
- Quote: "The VCs report that they spend an average of 22 hours per week networking and sourcing deals and an average of 18 hours per week working with portfolio companies out of a total reported workweek of 55 hours."

- Claim: LPs cannot access real-time data on investment value during a fund's lifetime; periodic GP updates are "smoothed by nature" and can lead to misleading valuations
- Relevant hypothesis: H5
- Source: Moonfare
- Date: Current
- URL: https://www.moonfare.com/pe-masterclass/optimising-pe-porftolio
- Quote: "Unlike public equity investors, Limited Partners cannot access real-time data on the value of investments in a fund during its lifetime. They rely instead upon periodic updates from the General Partner, which are smoothed by nature."

- Claim: Only **~50%** of PE funds currently adopt the ILPA Reporting Template
- Relevant hypothesis: H5
- Source: ILPA
- Date: January 2025
- URL: https://ilpa.org/news/how-we-got-to-new-reporting-standards/
- Quote: "This represents a marked improvement over the existing adoption rate for the 2016 template version, which sits about 50% currently."

- Claim: Anecdotal evidence suggests portfolio managers are spending significantly more time negotiating with lenders and overseeing operational issues, crowding out new deal activity
- Relevant hypothesis: H5
- Source: Bain & Company Private Equity Midyear Report 2024
- Date: 2024
- URL: https://www.bain.com/insights/private-equity-midyear-report-2024/
- Quote: "Fighting fires within portfolios consumes the time, energy, and confidence of both operating and senior investment professionals, which tends to check new activity."

**Assessment:** H5 is **strongly supported by structural evidence**. The combination of quarterly reporting cycles, 1-2 quarter data lag, majority-manual data collection processes, declining time allocation to monitoring, and growing portfolio scale creates a structural monitoring deficit. By the time formal reports reach decision-makers, the underlying data can be **4-6 months old**. The PwC finding that 54% of portfolio data exchange happens via email attachments — and 61% of reports are manually built — is the most concrete evidence of the operational gap.

However, the precise data point the hypotheses request — "hours per week portfolio managers spend compiling information" — **was not found in any published study**. The Shore Capital case study (Spaulding Ridge) noted that pre-automation firms spent "most of their time assembling data versus analyzing it," but no specific hour count exists. This is a notable evidence gap.

H5 is a **root problem** driven by structural information asymmetry in private markets. Unlike public companies with real-time pricing and mandatory disclosure, private portfolio companies have no obligation to provide continuous data, and most lack the infrastructure to do so even if required.

---

## H6: Startup founders lose weeks to due diligence during fundraising

- Claim: Fundraising requires **over 20 hours per week for 3 to 9 months**
- Relevant hypothesis: H6
- Source: Founder Institute
- Date: Ongoing
- URL: https://fi.co/startup-funding-checklist
- Quote: "It will take over 20 hours per week for anywhere from three to nine months. You will become frustrated many times and will think about giving up."

- Claim: During peak seed fundraising, Levels CEO Sam Corcos averaged **110 hours per week** working
- Relevant hypothesis: H6
- Source: First Round Review
- Date: March 2025
- URL: https://review.firstround.com/how-i-spent-17784-hours-in-5-years-as-a-startup-founder/
- Quote: "During the peak of our seed round fundraising, I averaged 110 hours a week."

- Claim: Close to **two-thirds of founders** spend at least 3 days per month on fundraising (~2 months per year). One-third spend 5+ days per month
- Relevant hypothesis: H6
- Source: BFA Global / Catalyst Fund (survey of 60+ founders)
- Date: April 2021
- URL: https://bfaglobal.com/catalyst-fund/insights/3-tips-to-help-startup-founders-decrease-time-spent-fundraising/
- Quote: "Close to two-thirds of founders spend at least three days per month on average fundraising. That amounts to close to two months spent each year, just on fundraising."

- Claim: In 2023, seed founders contacted an average of **66 investors** (up from 48 in 2022), with 50% of successful raises taking **13-24 weeks**
- Relevant hypothesis: H6
- Source: DocSend / Dropbox
- Date: December 7, 2023
- URL: https://www.prnewswire.com/news-releases/why-now-successful-founders-display-urgency-among-market-competition-in-docsends-annual-seed-report-302008136.html
- Quote: "With 50% of successful raises taking 13 to 24 weeks in 2023...The average number of investors contacted is 66 in 2023, up from 48 in 2022."

- Claim: Post-term-sheet DD takes **2-3 weeks minimum, up to 2 months**; well-prepared companies can complete in 2-4 weeks
- Relevant hypothesis: H6
- Source: DealRoom / Kruze Consulting
- Date: 2025 / 2024
- URL: https://dealroom.net/blog/startup-due-diligence / https://kruzeconsulting.com/blog/due-diligence-overview/
- Quote: "At a minimum, the process should take 2-3 weeks, and could extend up to 2 months." / "These companies typically can get through due diligence in two to four weeks."

- Claim: Drop (Massdrop) saw **revenue go to zero for eight consecutive weeks** during/after fundraising
- Relevant hypothesis: H6
- Source: First Round Review
- Date: Published on First Round Review
- URL: https://review.firstround.com/the-first-time-founders-guide-to-learning-everything-the-hard-way/
- Quote: "We did zero for eight consecutive weeks and we couldn't figure out why."

- Claim: Typical Series A data rooms include **30-50+ individual documents** across 8-12 major categories; 89% of investors now require VDR access
- Relevant hypothesis: H6
- Source: Multiple DD checklists; Visible.vc
- Date: 2024
- URL: https://visible.vc/blog/startup-data-room/
- Quote: "89% of investors now require secure digital access to due diligence materials via a virtual data room."

- Claim: For each 100 investment opportunities a VC reviews, approximately 10 receive detailed DD and the fund invests in just 1 — meaning founders go through many DD processes for few closings
- Relevant hypothesis: H6
- Source: MaRS Discovery District
- Date: 2024
- URL: https://learn.marsdd.com/article/the-due-diligence-process-in-venture-capital/
- Quote: "Typically, for each 100 opportunities reviewed, ten will receive a detailed review and the fund may invest in one of them."

- Claim: Median time from seed to Series A stretched to **774 days (~2.1 years)** in Q4 2024, up from 420 days in Q4 2021
- Relevant hypothesis: H6
- Source: Fundreef (citing Carta/Crunchbase data)
- Date: 2025
- URL: https://www.fundreef.com/resources/how-long-does-fundraising-take-timeline-by-stage/
- Quote: "The median time between seed and Series A stretched to 774 days (2.1 years) in late 2024."

**Assessment:** H6 is **strongly supported**, particularly by the DocSend longitudinal data showing fundraising timelines lengthening and the Founder Institute's 20+ hours/week estimate. The multiplicative burden is key: if each VC spends **20+ hours** on DD per potential investment (Affinity/Wiltbank data), and founders must support this process for dozens of investors, the total response burden is enormous — and mostly duplicated across investors asking for the same documents and answers independently. The Drop revenue-to-zero anecdote is a single case but illustrates the real business cost.

H6 is both a root problem and partly a **structural consequence** of how venture fundraising works. The deeper issue is that each investor independently conducts their own DD process with no standardization or information sharing between them, forcing founders to serve as the redundant integration layer across all these parallel processes. Notably, data rooms partially address the document duplication problem, but **the Q&A and response burden remains multiplicative** — each investor asks their own questions.

---

## What the cognitive science says about all six hypotheses

Academic research provides a strong theoretical foundation for why these pain points degrade decision quality:

- Claim: Four "Decision Stressors" — **information overload, time pressure, complexity, and uncertainty** — significantly impair decision quality; cognitive biases are amplified
- Relevant hypothesis: Academic context (all hypotheses)
- Source: Phillips-Wren & Adya, Journal of Decision Systems
- Date: May 2020
- URL: https://www.tandfonline.com/doi/abs/10.1080/12460125.2020.1768680

- Claim: Under time pressure, people rely more on heuristic processing, **gather less information, are less accurate**, and are less likely to revise initial impressions
- Relevant hypothesis: Academic context
- Source: Multiple academic sources compiled by Moore (2017)
- Date: Various
- URL: https://learnmoore.org/mooredata/TPND.pdf
- Quote: "Decision makers under time pressure tend to gather less information and act more quickly...more likely to rely on cognitive heuristics...are less accurate."

- Claim: An **inverted U-shaped relationship** between information load and decision quality occurs specifically when time pressure is present but does not occur without time pressure
- Relevant hypothesis: Academic context
- Source: Hahn, Lawson & Young, Psychology & Marketing
- Date: 1992
- URL: https://onlinelibrary.wiley.com/doi/abs/10.1002/mar.4220090503
- Quote: "An inverted U-shaped function relating decision quality to information load occurred when time pressure was present, but did not when it was absent."

- Claim: Information overload is positively related to **strain, burnout**, and negatively related to job satisfaction; associated with serious performance losses
- Relevant hypothesis: Academic context
- Source: Frontiers in Psychology (systematic review)
- Date: 2023
- URL: https://www.frontiersin.org/journals/psychology/articles/10.3389/fpsyg.2023.1122200/full
- Quote: "Information overload is associated with serious performance losses, especially in connection with disruptions and interruptions."

This body of research confirms that the conditions described in H1-H6 are not merely inconvenient — they **systematically degrade the quality of analytical judgment**. DD teams operating under the combination of massive document volumes and compressed timelines are, according to the cognitive science, making decisions with impaired analytical processing and elevated heuristic reliance.

---

## Unanticipated findings that no hypothesis captured

**New finding 1: AI adoption for M&A has doubled but remains below 50%.** Bain reports AI adoption for M&A more than doubled to 45% of practitioners in 2025, with 78% reporting productivity gains. This means **55% of the industry still operates without AI assistance** — the pain points described in H1-H4 remain the default experience for the majority.

- Source: Bain & Company
- Date: 2025
- URL: https://www.bain.com/insights/looking-back-m-and-a-report-2026/
- Quote: "AI adoption for M&A more than doubled, to 45% of practitioners...78% say they achieved productivity gains from reduced manual effort."

**New finding 2: The audit comfort paradox.** Financial statement breach claims are received at a *higher* rate for audited companies than unaudited ones. This suggests that the presence of an auditor's opinion creates false comfort that reduces DD scrutiny — a behavioral finding that complicates the assumption that more verification always helps.

**New finding 3: The Q&A bottleneck may matter more than document review.** Ansarada's finding that up to 70% of deal time is spent on Q&A — not initial document review — suggests the primary bottleneck may not be reading documents but rather the iterative process of asking and answering questions about what the documents say. This reframes the problem: the core challenge may be less about document scanning and more about **interactive knowledge extraction** from unstructured sources.

**New finding 4: Deal timelines are lengthening, not shortening.** Despite technology improvements, Datasite's platform data shows deal duration increasing 48% from 2021 to 2024. This contradicts the assumption that more tools would accelerate processes and suggests that **regulatory complexity and DD scope are expanding faster than tools can compress timelines**.

---

## Conclusion: the hierarchy of root causes

The six hypotheses are not independent problems — they form a causal chain. **H1 (volume under time pressure) is the foundational root cause**. It creates the conditions for H3 (search consuming analysis time), which in turn enables H2 (anomalies going undetected). H4 (fragmentation) is a structural multiplier that amplifies all three — when the same documents are reviewed by siloed teams who cannot synthesize findings, the effective processing capacity is lower than the nominal person-hours would suggest. H5 (portfolio monitoring gaps) reflects the same fundamental problem — too much unstructured information, too little time — transposed from the deal context to the portfolio management context. H6 (founder burden) is the demand-side mirror image: the same information asymmetry and lack of standardization that burdens DD teams also burdens the companies being evaluated.

Three evidence gaps deserve explicit acknowledgment. First, **no rigorous, independent time-motion study of DD analyst workflows exists** — the most specific time-allocation figures (37% searching, 90/10 processing-vs.-analysis) come from software vendors. Second, **no study quantifies duplicated effort across DD workstreams** (H4) in hours or percentage terms. Third, **no published data measures the exact hours portfolio managers spend compiling interim reports** (H5). These gaps represent opportunities for primary research that would strengthen or refine these hypotheses considerably.