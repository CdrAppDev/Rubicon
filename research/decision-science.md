# Why trained professionals miss what they're looking for

**Expert decision-making collapses predictably under information overload — and expertise itself creates new failure modes that make due diligence structurally unreliable.** Across medicine, auditing, intelligence analysis, legal review, and cognitive science, a convergent body of research demonstrates that when trained professionals process large volumes of information under time pressure, they miss critical findings at alarming rates, develop tunnel vision, anchor on early impressions, and stop searching prematurely. These are not failures of effort or competence. They are biological constraints of human information processing that no amount of training overcomes. The implications for M&A due diligence — which demands cross-document synthesis of thousands of unstructured pages within deal-driven timelines — are devastating.

---

## Professionals miss obvious findings while looking directly at them

The most striking evidence comes from radiology, where the phenomenon of **inattentional blindness** has been documented with unusual precision. Drew, Võ, and Wolfe (2013) inserted a gorilla image — **48 times larger than the average lung nodule** — into CT scans reviewed by 24 radiologists performing a routine nodule-detection task. **83% of radiologists failed to see the gorilla**, and eye-tracking confirmed that the majority of those who missed it looked directly at its location. The authors concluded: "Even expert searchers, operating in their domain of expertise, are vulnerable to inattentional blindness." A follow-up study by Williams et al. (2021) found that radiologists focused on lung nodules missed a **breast cancer mass approximately 80% of the time** — and a medical checklist did not mitigate the effect.

The magnitude of this effect scales with cognitive load. Simons et al. (2024), synthesizing decades of inattentional blindness research in medical contexts, found that under high-load conditions, only **8% of participants noticed unexpected objects**, compared to **47% under low load** — a roughly sixfold decrease in detection capacity. This finding has been replicated across professional populations: approximately 58% of police trainees and 33% of experienced officers failed to notice a gun visible on a car dashboard during simulated traffic stops (Simons & Schlosser, 2017). In aviation, Wickens and Alexander (2009) found that **17 of 24 pilots failed to detect critical events** when engaged with compelling displays under high workload, a phenomenon Wickens termed **attentional tunneling**.

For due diligence, these findings mean that lawyers organized by workstream — financial, legal, regulatory, operational — are structurally vulnerable to missing cross-domain risks that fall outside their assigned search pattern. A reviewer focused on revenue recognition may look directly at a page containing an environmental liability disclosure and literally not see it. The problem is not carelessness. It is the architecture of human attention under load.

---

## The single most common expert error is stopping too early

Across medicine, the most extensively documented cognitive failure is **premature closure** — the tendency to stop considering alternatives once an initial interpretation forms. Graber, Franklin, and Gordon (2005) analyzed 100 cases of diagnostic error in internal medicine and found that cognitive factors contributed to **74% of errors**, with premature closure identified as "the single most common cause." Critically, **faulty or inadequate knowledge was uncommon**. The errors arose not from ignorance but from how experts process information under load. As one widely cited formulation puts it: "When the diagnosis is made, the thinking stops."

A closely related phenomenon, **satisfaction of search**, has been studied in radiology for over 50 years. When radiologists find one abnormality, their detection rate for subsequent abnormalities drops significantly. Berbaum et al. (1990) demonstrated that finding one lesion produced **statistically significant reductions in perceptual accuracy** for additional lesions in the same image. A comprehensive review by Adamo et al. (2021) found that satisfaction-of-search errors account for **up to one-third of radiological reading errors**. Three competing theories explain the mechanism — satisfaction (observers stop searching), perceptual set (finding one target biases search toward similar targets), and resource depletion (detecting the first target consumes cognitive resources needed for subsequent detection) — and all three apply directly to document review contexts.

The audit literature documents the same pattern under the label **premature sign-off**. Alderman and Deitrick (1982), Kelley and Margheim (1990), and Otley and Pierce (1996) established that time budget pressure leads auditors to sign off on procedures without completing required work. Raghunathan (1991) analyzed this as a form of **satisficing** — auditors stop gathering evidence when they believe they have "enough," even when required procedures remain incomplete. Herbert Simon's (1955) foundational framework predicted exactly this: when constraints make optimal search impossible, decision-makers adopt aspiration-level thresholds and stop at the first satisfactory option. In due diligence, this manifests as teams accepting preliminary risk profiles rather than exhaustively evaluating all documents — a structural inevitability when thousands of pages face week-long timelines.

---

## Time pressure selectively destroys the ability to detect qualitative red flags

One of the most consequential findings for the due diligence thesis comes from Braun (2000), who studied 50 practicing auditors under varying time pressure. **Time-pressured auditors maintained accuracy on quantitative tick-mark documentation** — frequency and amount of misstatements — **but filtered out qualitative indicators of potential fraud**. Under low time pressure, auditors attended to a broader range of fraud cues and investigated them more extensively. Braun concluded that "time pressure's earliest effects may be manifested through the filtering out of cues related to qualitative aspects of misstatements."

This finding is devastating for due diligence because the most critical deal risks are typically qualitative: unusual contract terms, conditional obligations buried in agreements, inconsistencies between management representations and documentary evidence, related-party transactions. Due diligence teams under deal-driven timelines may accurately track quantitative metrics while completely missing the contextual warning signs that signal deal-breaking problems.

The pattern extends across domains. Pietsch and Messier (2017) found that high time pressure in accounting produces **primacy effects** — early evidence is overweighted while later evidence is underweighted. Some participants **ignored later information entirely** as a coping mechanism. Bennett and Hatfield (2017) found that auditors under pressure **reclassify detected misstatements as less material**, effectively downgrading problems to manage workload. Glover (1997) showed that time-pressured auditors lose the ability to differentiate diagnostic from nondiagnostic information, leading to **diluted judgments** where irrelevant detail carries equal weight to critical findings.

Croskerry's (2009, 2013) dual-process model explains the underlying mechanism. Type 1 (intuitive) processing dominates approximately **95% of clinical decisions** and is more error-prone. Type 2 (analytical) processing is reliable but slow. Under cognitive load, System 2 processing is **preferentially impaired**, forcing greater reliance on fast, heuristic-driven System 1. The conditions of due diligence — time pressure, fatigue, massive document volumes — are precisely the conditions Croskerry identifies as maximizing cognitive bias. And the solution his framework prescribes — metacognitive reflection, "slowing down" — is exactly what deal timelines prohibit.

---

## Human document reviewers agree with each other less than a third of the time

The legal e-discovery literature provides the most direct quantitative evidence for human unreliability in large-scale document review. Roitblat, Kershaw, and Oot (2010) studied two independent teams of professional reviewers assessing 5,000 documents from an actual M&A-context Second Request (the MCI/Verizon acquisition). **The two teams agreed on relevance determinations only 28% of the time.** Each team's overlap with the original production was even lower: **16.3% and 15.8%** respectively. The original review had cost **$14 million over four months of 100-hour weeks**. Both human teams achieved F1 scores of just **0.27 and 0.28**, while automated systems scored 0.34 and 0.38.

Blair and Maron (1985) found an even more troubling pattern in a real legal case involving 40,000 documents. Attorneys and experienced paralegals **believed they had retrieved over 75% of relevant documents but had actually found approximately 20%** — a 55-percentage-point calibration gap between confidence and performance. Voorhees (2000), analyzing NIST data across expert assessors, found **mean overlap of just 32.8%** on relevance judgments, establishing a practical upper bound on human retrieval performance at roughly **65% precision at 65% recall** — "since that is the level at which humans agree with one another." Grossman and Cormack (2011), using TREC Legal Track data, demonstrated that technology-assisted review achieved results **superior to exhaustive manual review** on both recall and precision.

Most directly relevant is Donnelly et al. (2019), who studied nine lawyers conducting simulated due diligence review of 50 contracts. Lawyers agreed on the general location of relevant material but **did not agree on the extent of relevant material**, and no strong differences emerged between lawyers with different levels of due diligence expertise. Different lawyers reviewing identical contracts identified different risks — meaning due diligence outcomes are partly a function of which lawyer happens to conduct the review.

---

## Expertise does not protect against overload — it creates overconfidence

A persistent assumption in deal practice is that senior professionals catch what juniors miss. The evidence contradicts this. Krupat et al. (2017) tested 75 participants — medical students, residents, and faculty — on complex clinical vignettes requiring them to resist premature closure. **There were no significant differences in diagnostic accuracy across the three experience levels.** Clinical experience did not protect against the cognitive mechanisms that cause miss errors. In radiology, Ashman et al. (2000) found that attending radiologists showed satisfaction-of-search effects "only slightly less" than lower-level residents. Cain et al. (2023) found that overall satisfaction-of-search rates **did not differ between novice and experienced searchers**.

Shanteau's (1992) seminal framework in *Organizational Behavior and Human Decision Processes* reconciled the contradictory findings on expert performance by showing that **expert competence depends on domain characteristics**. Expertise is reliably demonstrated in domains with predictable outcomes, clear feedback loops, and repeatable stimuli (weather forecasting, chess, livestock judging). It consistently fails in domains with unpredictable outcomes, delayed feedback, and unique stimuli. M&A due diligence shares the characteristics of domains where expertise fails: outcomes are unpredictable, feedback is delayed by years, and every deal presents unique document sets.

Dror (2020), publishing in *Analytical Chemistry*, went further: **"expertise and experience may actually increase (or even cause) certain biases."** Experts engage in more selective attention, rely more heavily on schemas and heuristics, and develop stronger expectations from prior experience. These top-down processes improve efficiency on routine tasks but create **a priori assumptions that blind experts to unexpected findings**. Dror identifies the "expert immunity" belief as one of six fallacies: "no one is immune to bias, not even experts... in many ways, experts are more susceptible to certain biases." Berthet (2022), reviewing evidence across management, finance, medicine, and law in *Frontiers in Psychology*, found that **overconfidence is the most recurrent bias** across all four professional domains.

Heuer's (1999) *Psychology of Intelligence Analysis* — the CIA's canonical text on analytic failure — arrived at the same conclusion from the intelligence domain. As Douglas MacEachin wrote in the foreword: **"When experts fall victim to these traps, the effects can be aggravated by the confidence that attaches to expertise — both in their own view and in the perception of others."** Heuer's most consequential finding was that additional information primarily increases analysts' **confidence** without proportionally improving their **accuracy** — creating a dangerous gap between perceived and actual analytical quality. This maps directly to the VDR model: adding more documents to a data room may increase the deal team's confidence without meaningfully improving their detection of material risks.

---

## Intelligence failures reveal the cross-synthesis problem at national scale

The intelligence community has produced the most extensive post-mortem literature on analytic failure under information overload, and the structural parallels to due diligence are exact. Roberta Wohlstetter's (1962) RAND analysis of Pearl Harbor established the foundational signal-noise framework: the attack was "presaged by a mass and variety of signals, which nonetheless achieved complete and overwhelming surprise." The failure was not insufficient intelligence but **a plethora of irrelevant information** that drowned the critical signals. The US government was "bombarded by so much irrelevant 'noise' that the true signals were buried."

The 9/11 Commission (2004) documented the same structural failure. The system was "blinking red" — over 40 intelligence articles in the President's Daily Briefs mentioned Bin Laden in the months before the attack. The FBI's Phoenix memo warning about suspicious flight school activity existed but was never synthesized with CIA database entries on identified terrorists. The Commission concluded that "the most important failure was one of imagination" and that the FBI "lacked the capacity to 'know what it knows' — to turn all the bits of intelligence streaming in from around the world into meaningful assessments."

The WMD Commission (Robb-Silberman, 2005) added the confirmation bias dimension. Analysts took a reasonable initial hypothesis — that Iraq likely possessed WMD — and **converted it into "a premise that was no longer subject to scrutiny."** They failed to ask the critical disconfirming question: "What would the evidence look like if Iraq did not have WMD programs?" The Commission found the intelligence community "dead wrong in almost all of its pre-war judgments," attributing the failure to "poor tradecraft and poor management" rather than political pressure.

Rob Johnston's (2005) CIA ethnographic study found **no baseline standard analytic method** in the intelligence community. Instead, "the most common practice is to conduct limited brainstorming on the basis of previous analysis, thus producing a bias toward confirming earlier views." The IC did "more reporting than in-depth analysis." RAND's review of group processes in intelligence analysis identified the **common knowledge effect** as particularly damaging: groups discuss shared information while unique information held by individual analysts "often fails to be introduced into the group discussion, and when it is mentioned, it is often overlooked."

Every one of these failure modes maps to due diligence. The VDR is the data room equivalent of the IC's collection systems — massive information input that overwhelms analytic capacity. The deal thesis functions as the intelligence hypothesis that hardens into an unfalsifiable premise. Siloed workstreams replicate the organizational fragmentation that prevented cross-agency synthesis before 9/11. And the junior analyst who notices an anomaly in an obscure contract faces the same dynamic as the FBI field agent whose Phoenix memo never reached decision-makers — unique, unshared information systematically fails to reach group-level synthesis.

---

## The audit profession documents systematic quality degradation under time pressure

The audit literature provides the most directly analogous evidence because auditing shares core structural features with due diligence: professional review of financial and documentary evidence under time constraints with high-stakes consequences. López and Peters (2012), studying 8,384 firm-year observations, found that busy-season workload compression produced **measurably greater abnormal accruals** and higher rates of earnings benchmark achievement — both indicators of degraded audit quality. Coram, Ng, and Woodliff (2003, 2004) established that time budget pressure has a direct positive relationship with **Reduced Audit Quality (RAQ) behaviors**: premature sign-offs, acceptance of weak client explanations, reduced sample sizes, and superficial document review. These behaviors persisted even when the perceived risk of misstatement was high.

DeZoort and Lord's (1997) seminal literature review in *Journal of Accounting Literature* established that workload pressure and time pressure are **independent variables that combine to degrade quality** — both present simultaneously in M&A due diligence. The review found that compromising audit quality is "one of the best possible ways auditors alleviate time budget pressure." Smith and Emerson (2017), surveying 258 auditors across seven of the ten largest US accounting firms, confirmed that **RAQ practices "still represent a serious issue for the profession"** despite decades of regulatory attention and firm-level quality controls.

Cassell, Dearden, Rosser, and Shipman (2022) provided rare archival (not merely experimental) evidence that **confirmation bias survives audit quality control processes**: auditors with experience on low-risk clients failed to adequately respond when risk levels increased, and this effect was only mitigated when the risk increase was dramatic enough to violate "reasonableness constraints." Durkin and Rose (2025) extended this to show that time pressure specifically promotes confirmation of client assertions, with auditors becoming "less likely to attend to evidence that disconfirms management's assertions." In the due diligence context, this means deal teams processing documents under compressed timelines will systematically underweight evidence contradicting the investment thesis — precisely the information that matters most.

---

## Cognitive architecture imposes hard limits that due diligence exceeds

Working memory capacity — the cognitive workspace where information is held, combined, and evaluated — is now understood as approximately **4 ± 1 items**, revised downward from Miller's (1956) classic 7 ± 2 estimate. Iselin (1989) found that decision performance falls when information cues exceed approximately **10 items**, with the effect amplified by information diversity. Experts partially compensate through chunking — organizing information into larger meaningful units — but this advantage breaks down when information exceeds available schemas or requires novel cross-domain synthesis. Due diligence demands holding multiple unrelated data points across legal, financial, and operational dimensions simultaneously, precisely the configuration where chunking fails.

Endsley's (1995) situation awareness model, published in *Human Factors*, identifies attention and working memory as **critical bottleneck factors** limiting professionals' ability to acquire, process, and integrate environmental information. Her research found that **approximately 18% of situation awareness errors** traced to mental model problems — including using incorrect mental models (6.5%) and over-reliance on default values (4.6%). Wickens' (2008) Multiple Resource Theory predicts maximum interference when tasks compete for the same processing resources — and due diligence review tasks are predominantly visual-verbal, creating resource competition across documents.

Kalyuga, Ayres, Chandler, and Sweller (2003) identified the **expertise reversal effect**: instructional scaffolding designed for novices becomes extraneous cognitive load for experts, actually impairing their performance. Applied to due diligence, this suggests that exhaustive checklists and redundant review procedures designed to ensure junior-reviewer thoroughness may paradoxically harm senior reviewers by consuming cognitive resources that would otherwise support germane analytical processing.

Klein's (1993) Recognition-Primed Decision model showed that experienced professionals typically recognize situations as familiar and identify a single course of action rather than analytically comparing options — effective in domains with repeatable patterns and clear feedback, but dangerous when applied to genuinely novel situations. Every M&A deal presents a unique combination of target company documents, limiting the pattern-matching that RPD relies on. Due diligence professionals may believe they are exercising expert judgment when they are actually applying inappropriate pattern recognition to novel circumstances.

---

## Conclusion: the failure is structural, not individual

The research across six professional domains converges on a single conclusion: **M&A due diligence demands cognitive performance that human information processing systems cannot deliver**. This is not a claim about professional competence. Radiologists, auditors, intelligence analysts, and lawyers are highly trained, deeply motivated, and working within established professional frameworks — and they still miss 10-30% of critical findings under normal conditions, agree with each other only 28-33% of the time on document relevance, and suffer 6x detection rate declines under high cognitive load.

Three structural features make due diligence particularly vulnerable. First, **the signal-noise ratio is catastrophic**: material risks are rare events distributed across thousands of routine pages, replicating the low-prevalence conditions that Evans et al. (2013) showed produce 2.5x higher miss rates in medical screening. Second, **cross-document synthesis has no cognitive shortcut**: unlike single-document review, identifying risks that only become apparent when information from multiple sources is combined requires simultaneous working memory processing that exceeds the 4±1 item limit regardless of expertise. Third, **deal momentum creates compound pressure**: time constraints force System 1 processing (Kahneman), the investment thesis creates confirmation bias (Cassell et al., 2022), and deal-completion incentives produce the same moral hazard dynamics Zacks (2015) identified in contract review — all operating simultaneously.

Medicine has spent 60 years documenting these problems and has not solved them through training, checklists, or process improvements alone. The intelligence community invested billions in structured analytic techniques that Mandel and Tetlock concluded "no one knows" whether they actually work. The audit profession, despite regulatory oversight and institutional quality controls, still reports "serious" reduced-quality practices. The evidence base does not support the proposition that due diligence — less structured, more time-compressed, and less regulated than any of these domains — can overcome the same cognitive limitations through incremental process improvements. The constraint is architectural. The solution must be, too.